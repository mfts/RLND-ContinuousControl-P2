{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies\n",
    "Most importantly install [Unity ML-agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md), PyTorch, and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the Unity enviroment downloaded and change the path of the file_name\n",
    "\n",
    "We are using the **20-agent environment** of Reacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exame the State and Action Spaces\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate and initialize the agent\n",
    "The learning agent is imported from a separate file \"./agent.py\" and takes `state_size`, `action_size` and a `seed` as instance variables.\n",
    "\n",
    "A few highlights of the agent:\n",
    "- The agent selects the policy given by the actor-critic network\n",
    "- The agent uses a buffer to store recent steps `(state, action, reward, next_state, done)` tuples and replay them\n",
    "- The agent maximizes reward based on an actor-critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "\n",
    "BUFFER_SIZE = int(1e6)  # size of the memory buffer\n",
    "BATCH_SIZE = 128        # sample minibatch size\n",
    "GAMMA = 0.99            # discount rate for future rewards\n",
    "LR_ACTOR = 1e-4         # learning rate of Actor\n",
    "LR_CRITIC = 1e-4        # learning rate of Critic\n",
    "TAU = 1e-3              # interpolation factor for soft update of target network\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "UPDATE_EVERY = 20       # update every 20 time steps\n",
    "NUM_UPDATES = 10        # number of updates to the network\n",
    "\n",
    "state_size = env_info.vector_observations.shape[1]  # size of the observation space (state space)\n",
    "action_size = brain.vector_action_space_size        # size of the action space\n",
    "num_agents = len(env_info.agents)                   # number of agents\n",
    "\n",
    "agent = Agent(state_size, \n",
    "              action_size, \n",
    "              BUFFER_SIZE, \n",
    "              BATCH_SIZE, \n",
    "              num_agents,\n",
    "              seed=0,\n",
    "              gamma=GAMMA,\n",
    "              tau=TAU,\n",
    "              lr_actor=LR_ACTOR,\n",
    "              lr_critic=LR_CRITIC,\n",
    "              weight_decay=WEIGHT_DECAY,\n",
    "              update_every=UPDATE_EVERY,\n",
    "              num_updates=NUM_UPDATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test randomly selected actions (untrained agent)\n",
    "Run randomly selected actions in the environment to see what happens to the score. This is similar to an **untrained** agent.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step. A window should pop up that allows you to observe the agents, as it moves through the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train multiple agents with Deep Determinitic Policy Gradient (DDPG)\n",
    "The agent actually runs on an underlying actor-critic network. This is beneficial instead of using an typical deep Q-learning network (DQN) not only the environment's state space is large at 33 variables but the action space contains 4 continuous action variables. \n",
    "\n",
    "The setup is similar to the DQN with a local and target network; however, now there are separate networks to evaluate: the **actor** network for learning the optimal policy and the **critic** network for evaluating the selected action. \n",
    "\n",
    "Let's train the agents until they achieve an average score of +30 over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000):\n",
    "    '''\n",
    "    -------------------------------------------\n",
    "    Parameters\n",
    "    \n",
    "    n_episodes: # of episodes that the agent is training for\n",
    "    max_t:      # of time steps (max) the agent is taking per episode\n",
    "    -------------------------------------------\n",
    "    '''\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]            # turn on train mode of the environment\n",
    "        states = env_info.vector_observations                        # get the current state for each agent\n",
    "        agent.reset()                                                # reset the OU noise parameter \n",
    "        ep_scores = np.zeros(num_agents)                             # initialize the score for each agent\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)                              # select an action for each agent \n",
    "            env_info = env.step(actions)[brain_name]                 # send all actions to the environment\n",
    "            next_states = env_info.vector_observations               # get next state for each agent\n",
    "            rewards = env_info.rewards                               # get reward for each agent\n",
    "            dones = env_info.local_done                              # check if episode finished\n",
    "            agent.step(states, actions, rewards, next_states, dones) # agents record enviroment response in recent step\n",
    "            states = next_states                                     # set the state as the next state for the following step for each agent\n",
    "            ep_scores += rewards                                     # update the total score\n",
    "            if np.any(dones):                                        # exit loop if episode for any agent finished\n",
    "                break \n",
    "                \n",
    "        scores_deque.append(np.mean(ep_scores))\n",
    "        scores.append(ep_scores)\n",
    "        \n",
    "        # print average epsiode score and average 100-episode score for each episode\n",
    "        print('\\rEpisode {} \\tScore: {:.2f} \\tAverage Score: {:.2f}'.format(i_episode, np.mean(ep_scores), np.mean(scores_deque)), end=\"\")  \n",
    "        \n",
    "        # print and save actor and critic weights when a score of +30 over 100 episodes has been achieved\n",
    "        if np.mean(scores_deque) >= 30.0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_final.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_final.pth')\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break    \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the scores\n",
    "Plot the scores according to their episodes. We can see a gradual increase in the scores as we increase the training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# plot scores\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "# plot average score/episode\n",
    "plt.plot(np.arange(1, len(scores)+1), [np.mean(s_ep) for s_ep in scores], c='black', linewidth=2)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test a trained agent\n",
    "Run a **trained** agent for 1000 time steps to see what happens to the score. Compare this with the score of the untrained agent from 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load policy network weights saved from training\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor_2.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False,)[brain_name] # reset enviroment and turn off training mode\n",
    "states = env_info.vector_observations               # get first states from the resetted enviroment\n",
    "scores = np.zeros(num_agents)                       \n",
    "\n",
    "for t in range(1000):\n",
    "    actions = agent.act(states, add_noise=False)     # select an action for each agent \n",
    "    env_info = env.step(actions)[brain_name]         # send all actions to the environment\n",
    "    next_states = env_info.vector_observations       # get next state for each agent\n",
    "    rewards = env_info.rewards                       # get reward for each agent\n",
    "    dones = env_info.local_done                      # check if episode finished\n",
    "    scores += rewards                                # set the state as the next state for the following step for each agent\n",
    "    states = next_states                             # update the total score\n",
    "    if np.any(dones):                                # exit loop if episode for any agent finished\n",
    "        break\n",
    "    \n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
